{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a33ad0-bb5a-468a-b357-b31f27b0cdeb",
   "metadata": {},
   "source": [
    "# Extracting metrics and plots from data repositories\n",
    "\n",
    "Last verified to run: 30 Jan 2025\n",
    "\n",
    "LSST Science Pipelines version: weekly 2025_04\n",
    "\n",
    "Contact authors: Jeff Carlin, Peter Ferguson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4833ab-bdf1-4df5-ab0e-f1b59152401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.butler import Butler\n",
    "\n",
    "from astropy.table import Table\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c8985-5feb-4321-98e5-908b78fe3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(metric_bundle_name, task_frame, butler):\n",
    "    \"\"\"Get the configuration corresponding to a MetricMeasurementBundle\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric_bundle_name: `string`\n",
    "        Name of the metric bundle to get config for\n",
    "    task_frame: `DataFrame`\n",
    "        DataFrame consisting of \"task_names\", \"task_type\", \"pipelines_names\", and \"pipelines_url\"\n",
    "    butler: `lsst.daf.Butler`\n",
    "        Butler initialized for the repo/collection of interest\n",
    "    \"\"\"\n",
    "    # Define the dataIds here rather than requiring them to be passed to this function:\n",
    "    did_tract = {'tract': 5063, 'skymap': 'lsst_cells_v1', 'band': 'g', 'instrument': 'LSSTComCam'}\n",
    "    did_visit_det = {'visit': 2024120500075, 'skymap': 'lsst_cells_v1', 'instrument': 'LSSTComCam', 'detector': 4}\n",
    "\n",
    "    # Extract the task name by string manipulation\n",
    "    task_name = str.split(metric_bundle_name, 'Metric')[0]\n",
    "    # Extract the pipeline name from the task_frame dict\n",
    "    pipeline_name = task_frame[task_frame.name == task_name].pipelines_name\n",
    "    config_name = pipeline_name.values[0]+'_config'\n",
    "\n",
    "    task_type = task_frame[task_frame.name == task_name].type\n",
    "\n",
    "    if 'tract' in task_type:\n",
    "        uri = butler.getURI(config_name, dataId=did_tract) # , collections=collection)\n",
    "    else:\n",
    "        uri = butler.getURI(config_name, dataId=did_visit_det) # , collections=collection)\n",
    "    config_string = uri.read().decode('utf-8')\n",
    "\n",
    "    return config_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06880329-9ed7-44c7-a6ba-14a89f8cfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(meas_bundle):\n",
    "    \"\"\"Extract metrics from an input metric bundle and print them to the screen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meas_bundle: `MetricsMeasurementBundle`\n",
    "        metric bundle from the Butler repo\n",
    "    \"\"\"\n",
    "    for key in meas_bundle.data.keys():\n",
    "        print(key, '\\n')\n",
    "        metrics_tmp = [m for m in meas_bundle[key]]\n",
    "        for met in metrics_tmp:\n",
    "            print(met)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def extract_metrics(meas_bundle):\n",
    "    \"\"\"Extract metrics from an input metric bundle and return as lists\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meas_bundle: `MetricsMeasurementBundle`\n",
    "        metric bundle from the Butler repo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    names: `list` of `strings`\n",
    "        Metric names\n",
    "    values: `list` of `floats`\n",
    "        Measured metric values\n",
    "    units: `list` of `units`\n",
    "        Units associated with each metric\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    values = []\n",
    "    units = []\n",
    "    for key in meas_bundle.data.keys():\n",
    "        for m in meas_bundle[key]:\n",
    "            names.append(key+'_'+m.metric_name.metric)\n",
    "            values.append(m.quantity.value)\n",
    "            units.append(m.quantity.unit)\n",
    "    return names, values, units\n",
    "\n",
    "\n",
    "def extract_metrics_table(meas_bundle):\n",
    "    \"\"\"Extract metrics from an input metric bundle and return as a Table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    meas_bundle: `MetricsMeasurementBundle`\n",
    "        metric bundle from the Butler repo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tab: `Astropy Table`\n",
    "        Table with columns of \"metric\", \"value\", and \"unit\" for all metrics\n",
    "        from the input bundle\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    values = []\n",
    "    units = []\n",
    "    for key in meas_bundle.data.keys():\n",
    "        for m in meas_bundle[key]:\n",
    "            names.append(key+'_'+m.metric_name.metric)\n",
    "            values.append(m.quantity.value)\n",
    "            units.append(m.quantity.unit)\n",
    "    tab = Table([names, values, units], names=['metric', 'value', 'unit'])\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564c76c-be38-4ae3-aae8-6bc8ecf77131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docstrings_for_task(atools_pipeline_config, bundles_list):\n",
    "    \"\"\"Extract the docstrings for a task associated with the input bundles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atools_pipeline_config: `config`\n",
    "        Pipeline config extracted from the Butler. (Could be extracted using \"get_config\")\n",
    "    bundles_list: `list`\n",
    "        A list of the MetricMeasurementBundles created by the task of interest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    docstrings_dict: `dict`\n",
    "        Dict containing the docstrings for each bundle, keyed on the bundle name\n",
    "    \"\"\"\n",
    "    docstring_dict = {}\n",
    "    for bun in bundles_list:\n",
    "        # Tasks are always specified in pipelines by assigning them via something like\n",
    "        # \"atools.stellarPhotometricRepeatability: StellarPhotometricRepeatability\"\n",
    "        # will appear in the config as bundle=task; for example:\n",
    "        # \"config.atools.stellarPhotometricRepeatability=lsst.analysis.tools.atools.StellarPhotometricRepeatability\"\n",
    "        # where the bundle will be \"stellarPhotometricRepeatability\", and the task is\n",
    "        # prepended with the path to the analysis_tools, \"lsst.analysis.tools.atools\".\n",
    "        sub_str = 'config.atools.'+bun+'=lsst.analysis.tools.atools'\n",
    "\n",
    "        # Find all instances of the substring, which should be at the start of the definition of any atool:\n",
    "        res = [i.start() for i in re.finditer(sub_str, atools_pipeline_config)]\n",
    "\n",
    "        for r in res:\n",
    "            # Process the string to extract the bundle name, module, and task:\n",
    "            full_string = atools_pipeline_config[r: r+150]\n",
    "            split0 = str.split(full_string, 'config.atools.')\n",
    "            bundle_name = str.split(split0[1], '=')[0]\n",
    "            split1 = str.split(split0[1], sub_str)\n",
    "            split2 = str.split(split1[0], '=')\n",
    "            last_period = split2[1].rfind('.')\n",
    "            module = split2[1][:last_period]\n",
    "            task = split2[1][last_period+1:][:-3]\n",
    "\n",
    "            # Import the module from the task and extract the docstring\n",
    "            task_dict = {}\n",
    "            exec(f\"from {module} import {task} as tmp_task\", task_dict)\n",
    "            # print(task_dict['tmp_task'].__name__)\n",
    "            doc = task_dict['tmp_task'].__doc__\n",
    "            if doc is not None:\n",
    "                doc0 = task_dict['tmp_task'].__doc__.replace('\\n', '')\n",
    "                doc1 = doc0.replace('  ', ' ')\n",
    "                doc2 = doc1.replace('  ', ' ')\n",
    "            else:\n",
    "                doc2 = doc\n",
    "\n",
    "            # Link to w_2025_02 version of the docs on github:\n",
    "            mod_new = module.replace('.', '/')\n",
    "            path = 'https://github.com/lsst/analysis_tools/blob/w.2025.02/python/'+mod_new+'.py'\n",
    "            info = {'docstring': doc2, 'path': path}\n",
    "            docstring_dict[bundle_name] = info\n",
    "\n",
    "    return docstring_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce8c3f-0659-43b2-a6ec-61b58a58b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_names_markdown_table(metric_bundle, metric_bundle_name, task_frame, butler, nodocs=False):\n",
    "    \"\"\"Function to extract related info for MetricMeasurementBundles and print a table formatted for markdown.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    metric_bundle: `MetricMeasurementBundle`\n",
    "        metric bundle from the Butler repo\n",
    "    metric_bundle_name: `string`\n",
    "        name of the metric bundle (usually \"TaskName\"+\"Metrics\")\n",
    "    task_frame: `DataFrame`\n",
    "        DataFrame consisting of \"task_names\", \"task_type\", \"pipelines_names\", and \"pipelines_url\"\n",
    "    butler: `lsst.daf.Butler`\n",
    "        Butler initialized for the repo/collection of interest\n",
    "    nodocs: `boolean` (default: False)\n",
    "        Set to True to turn off looking up of docstrings (useful in the case where it is known\n",
    "        that there are no docstrings)\n",
    "    \"\"\"\n",
    "    metrics_extract = extract_metrics(metric_bundle)\n",
    "    metric_names = metrics_extract[0][:]\n",
    "\n",
    "    bundles = []\n",
    "    mets = []\n",
    "\n",
    "    for met in metric_names:\n",
    "        bundle_name = met[:met.index(\"_\")]\n",
    "        bundles.append(bundle_name)\n",
    "        metric_name = met[met.index(\"_\")+1:]\n",
    "        mets.append(metric_name)\n",
    "\n",
    "    bundles = np.array(bundles)\n",
    "    mets = np.array(mets)\n",
    "\n",
    "    # Extract a list of unique bundle names\n",
    "    uniq_bundles = np.unique(bundles)\n",
    "\n",
    "    if nodocs is False:\n",
    "        # Retrieve the config for the task:\n",
    "        cfg = get_config(metric_bundle_name, task_frame, butler)\n",
    "        # Use the config to get docstrings for each bundle:\n",
    "        docstrings_dict = get_docstrings_for_task(cfg, uniq_bundles.tolist())\n",
    "\n",
    "    # Print a table formatted for a Markdown document (e.g., a TechNote)\n",
    "    for bun in uniq_bundles:\n",
    "        if nodocs is False:\n",
    "            link = f\"([source]({docstrings_dict[bun]['path']}))\"\n",
    "            print(f'\\nmetricBundle: **{bun}** {link}\\n')\n",
    "            if docstrings_dict[bun]['docstring'] is None:\n",
    "                print('_Docstring:_ No docstring\\n')\n",
    "            else:\n",
    "                print('_Docstring:_ '+docstrings_dict[bun]['docstring']+'\\n')\n",
    "        print('| metric name | value | units |\\n| ---  |--- |--- |')\n",
    "        inbundle = np.where(bundles == bun)\n",
    "        for i in inbundle[0]:\n",
    "            if metrics_extract[2][i] == 'ct':\n",
    "                print(f'| {mets[i]} | {metrics_extract[1][i]:.1f} | {metrics_extract[2][i]} |')\n",
    "            else:\n",
    "                print(f'| {mets[i]} | {metrics_extract[1][i]:.6f} | {metrics_extract[2][i]} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213f878-0c47-495f-a34b-b77b9b118fd0",
   "metadata": {},
   "source": [
    "## Metrics from the DRP run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e689a-1a3e-4fd0-be1d-a0fbabf71f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/repo/embargo'\n",
    "\n",
    "collections = ['LSSTComCam/runs/DRP/DP1-RC1/w_2025_02/DM-48371']\n",
    "\n",
    "butler = Butler(repo, collections=collections)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb22f80-53d3-408b-af13-8a214dc27951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which dataset types with storage class \"MetricMeasurementBundle\" exist in the collection\n",
    "metrics_datasets = []\n",
    "\n",
    "for datasetType in registry.queryDatasetTypes():\n",
    "    if registry.queryDatasets(datasetType, collections=collections).any(execute=False, exact=False):\n",
    "        if datasetType.storageClass_name == 'MetricMeasurementBundle':\n",
    "            print(datasetType)\n",
    "            metrics_datasets.append(datasetType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60546cbc-f431-40ed-bddc-32b649aa9edf",
   "metadata": {},
   "source": [
    "### Make a DataFrame to map task names, types, URLs, and pipeline names.\n",
    "\n",
    "(Ideally this could be done programmatically in the future, but for now it is manually created.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963760aa-5094-4708-88f2-e512d8f60230",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = [\n",
    "    'calexpSummary',\n",
    "    'diaSourceTableTract',\n",
    "    'matchedVisitCore',\n",
    "    'objectTableColumnValidate',\n",
    "    'objectTableCore',\n",
    "    'objectTableRefCat',\n",
    "    'objectTablePhotomRefCat',\n",
    "    'objectTableExtended',\n",
    "    'preSourceTableCore',\n",
    "    'propertyMapTract',\n",
    "    'sourceTable_visit_gaia_dr3_20230707_match'\n",
    "]\n",
    "task_type = [\n",
    "    \"visit\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"tract\",\n",
    "    \"visit\",\n",
    "    \"tract\",\n",
    "    \"visit\",\n",
    "]\n",
    "pipelines_names = [\n",
    "    'calibrate',\n",
    "    'analyzeDiaSourceTableTract',\n",
    "    'analyzeMatchedVisitCore',\n",
    "    'validateObjectTableCore',\n",
    "    'analyzeObjectTableCore',\n",
    "    'refCatObjectTract',\n",
    "    'photometricRefCatObjectTract',\n",
    "    'analyzeObjectTableExtended',\n",
    "    'analyzePreSourceTableCore',\n",
    "    'propertyMapTract',\n",
    "    'sourceTable_visit_gaia_dr3_20230707_match',\n",
    "]\n",
    "pipelines_url = [\n",
    "    'https://github.com/lsst/drp_pipe/blob/w.2025.02/pipelines/LSSTComCamSim/nightly-validation-ops-rehearsal-3.yaml#L18-L26',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/diaTractQualityCore.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/matchedVisitQualityCore.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddColumnValidate.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddQualityCore.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddQualityCore.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddQualityCore.yaml',\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddQualityExtended.yaml',\n",
    "    'https://github.com/lsst/drp_pipe/blob/w.2025.02/pipelines/_ingredients/LSSTComCamSim/DRP.yaml#L68-L78',\n",
    "    # TO DO: put in real links here\n",
    "    'https://github.com/lsst/analysis_tools/blob/w.2025.02/pipelines/coaddQualityCore.yaml#L90-L123',\n",
    "    'NA'\n",
    "]\n",
    "task_frame = pd.DataFrame({'name': task_names,\n",
    "                           'type': task_type, \n",
    "                           'pipelines_name': pipelines_names,\n",
    "                           'pipelines_url': pipelines_url})\n",
    "\n",
    "task_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5abc17-0e51-49aa-8277-855d32c590c7",
   "metadata": {},
   "source": [
    "### Create dataIds to use for data lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67d609-058b-4af9-a3ec-a751df2842e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataIds here rather than requiring them to be passed to this function:\n",
    "did_tract = {'tract': 5063, 'skymap': 'lsst_cells_v1', 'band': 'g', 'instrument': 'LSSTComCam'}\n",
    "did_patch = {'tract': 5063, 'skymap': 'lsst_cells_v1', 'band': 'g', 'instrument': 'LSSTComCam', 'patch':14}\n",
    "did_visit = {'visit': 2024120500075, 'skymap': 'lsst_cells_v1', 'instrument': 'LSSTComCam'}\n",
    "did_visit_det = {'visit': 2024120500075, 'skymap': 'lsst_cells_v1', 'instrument': 'LSSTComCam', 'detector': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22f11c-4922-44bd-a01f-105805f88116",
   "metadata": {},
   "source": [
    "## Extract the lists of metric names to a table that can be added to a markdown TechNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186e24e-a5d7-4853-8ba3-e554dc8df1ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in task_frame.name:\n",
    "    print('Extracting info for ', task)\n",
    "\n",
    "    # Choose the correct dataId depending on the \"type\" entry in task_frame:\n",
    "    if (task_frame[task_frame.name == task].type == 'tract').values[0]:\n",
    "        did = did_tract\n",
    "    else:\n",
    "        did = did_visit\n",
    "\n",
    "    # Wrap in a try/except so it doesn't fail when datasets don't exist\n",
    "    try:\n",
    "        # Extract the metric bundle\n",
    "        metrics = butler.get(task+'_metrics', collections=collections, dataId=did)\n",
    "        # Bundle names always end with \"Metrics\"\n",
    "        metric_bundle_name = task+'Metrics'\n",
    "        # calexpSummary and preSourceTableCore don't seem to have docs, so don't bother looking (to avoid errors)\n",
    "        if (task == 'calexpSummary') | (task == 'preSourceTableCore'):\n",
    "            print_metric_names_markdown_table(metrics, metric_bundle_name, task_frame, butler, nodocs=True)\n",
    "        else:\n",
    "            print_metric_names_markdown_table(metrics, metric_bundle_name, task_frame, butler)\n",
    "    except:\n",
    "        print('No metric bundles found for ', task)\n",
    "\n",
    "    print('\\n\\n------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245b1c4-44f7-4873-82c1-df6ddcea9d09",
   "metadata": {},
   "source": [
    "#### To see an example of extracting Astropy tables instead, uncomment and execute the following cell\n",
    "\n",
    "(Note that \"extract_metrics_table\" could be replaced with \"print_metrics_table\" if desired.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befddec-44f0-40ac-9b34-ca4f0a21a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for task in task_frame.name:\n",
    "    print('\\nExtracting info for ', task, '\\n')\n",
    "\n",
    "    # Choose the correct dataId depending on the \"type\" entry in task_frame:\n",
    "    if (task_frame[task_frame.name == task].type == 'tract').values[0]:\n",
    "        did = did_tract\n",
    "    else:\n",
    "        did = did_visit_det\n",
    "\n",
    "    # Wrap in a try/except so it doesn't fail when datasets don't exist\n",
    "    try:\n",
    "        # Extract the metric bundle\n",
    "        metrics = butler.get(task+'_metrics', collections=collections, dataId=did)\n",
    "        tab = extract_metrics_table(metrics)\n",
    "        print(tab)\n",
    "    except:\n",
    "        print('No metric bundles found for ', task)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a07cc3-d77f-4fbc-a144-63430146c9c8",
   "metadata": {},
   "source": [
    "## Extract task info and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a83778-935a-4308-93f4-f5de623f5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = registry.queryDatasetTypes()\n",
    "# grab plots in our collections\n",
    "plot_datasets=[ds for ds in datasets if ds.storageClass_name == 'Plot']\n",
    "plot_datasets=[ds for ds in plot_datasets if registry.queryDatasets(ds, collections=collections).any(execute=False, exact=False)]\n",
    "\n",
    "# only use examples with g-band or no band\n",
    "example_datasets=[]\n",
    "for ds in plot_datasets:\n",
    "    exampleBool=True\n",
    "    for band in [\"u\",\"r\",\"i\",\"z\",\"y\"]:\n",
    "        if f\"_{band}_\" in ds.name:\n",
    "            exampleBool=False\n",
    "    if exampleBool:\n",
    "        example_datasets.append(ds)\n",
    "        \n",
    "plot_datasets=example_datasets\n",
    "\n",
    "metric_datasets=[ds for ds in datasets if ds.storageClass_name == 'MetricMeasurementBundle']\n",
    "metric_datasets=[ds for ds in metric_datasets if registry.queryDatasets(ds, collections=collections).any(execute=False, exact=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab1946-bb59-4f26-b13e-620acad53ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [ds.name[:ds.name.find('_')] for ds in plot_datasets]\n",
    "tasks +=[ds.name[:ds.name.rfind('_')] for ds in metric_datasets]\n",
    "tasks =sorted(np.unique(tasks))\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abafc3-d57a-41d5-9474-46bbd25d1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_plot(plot_name,dataId, collections,butler):\n",
    "    \n",
    "    try:\n",
    "        ref = butler.registry.findDataset(plot_name, dataId, collections=collections)\n",
    "        uri = butler.getURI(ref)\n",
    "    except:\n",
    "        return\n",
    "    image_bytes = uri.read()\n",
    "    file_name = uri.basename()\n",
    "    task_name = file_name[:file_name.find('_')]\n",
    "    if \"Plot_\" in file_name:\n",
    "        str_rfind=\"Plot_\"\n",
    "        offset = 4\n",
    "    elif \"TwoHists_\" in file_name:\n",
    "        str_rfind=\"TwoHists_\"\n",
    "        offset = 8\n",
    "    else: \n",
    "        raise Exception(f\"bad filename {file_name}\")\n",
    "    file_name = file_name[:file_name.rfind(str_rfind)+offset]\n",
    "    file_name = file_name.lower().replace('_g_','_{band}_').replace(\"_\",\"-\")\n",
    "    base_directory = '../_static/plots/' + task_name +'/'\n",
    "    file_path = base_directory + file_name + '.png'\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(image_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c170971-daf3-447c-af5b-23917d139c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_table_dict={}\n",
    "single_task_dict={col:[] for col in [\"name\", \"original_name\", \"storage_class\", \"link\"]}\n",
    "\n",
    "# The following statement is a workaround for photometricRefCatObjectTract and refCatObjectTract,\n",
    "# which output files with \"objectTable_tract...\" at their beginning because the outputName\n",
    "# was not specified in coaddQualityCore.yaml\n",
    "os.makedirs('../_static/plots/objectTable', exist_ok=True)\n",
    "\n",
    "for i, row in task_frame.iterrows():\n",
    "    task = row['name']\n",
    "    print(task)\n",
    "    os.makedirs('../_static/plots/' + task, exist_ok=True)\n",
    "    task_table_dict[task]=copy.deepcopy(single_task_dict)\n",
    "    for ds in metric_datasets:\n",
    "        if ds.name[:ds.name.rfind('_')] in task:\n",
    "            task_table_dict[task][\"name\"].append(ds.name)\n",
    "            task_table_dict[task][\"original_name\"].append(ds.name)\n",
    "            task_table_dict[task][\"storage_class\"].append(ds.storageClass_name)\n",
    "            task_table_dict[task][\"link\"].append(ds.name.lower())\n",
    "    for ds in plot_datasets:\n",
    "        if ds.name[:ds.name.find('_')] in task:\n",
    "            task_table_dict[task][\"name\"].append(ds.name.replace('_g_','_{band}_'))\n",
    "            task_table_dict[task][\"original_name\"].append(ds.name)\n",
    "            task_table_dict[task][\"storage_class\"].append(ds.storageClass_name)\n",
    "            task_table_dict[task][\"link\"].append(ds.name.lower().replace('_g_','_{band}_').replace(\"_\",\"-\"))\n",
    "\n",
    "            # I was lazy and did try/except statements to make this work\n",
    "            write_plot(plot_name=ds.name, \n",
    "                       dataId = did_tract,\n",
    "                       collections=collections,\n",
    "                       butler=butler)\n",
    "            \n",
    "            write_plot(plot_name=ds.name, \n",
    "                       dataId = did_visit,\n",
    "                       collections=collections,\n",
    "                       butler=butler)\n",
    "    task_table_dict[task] = pd.DataFrame(task_table_dict[task])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af12d94-9a43-4d70-bf0f-0c9e8552457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_row(task,row):\n",
    "    if row['storage_class'] == \"Plot\":\n",
    "        str = f\"| [{task['pipelines_name']}]({task['pipelines_url']})|\"\n",
    "        str +=\"{ref}`\"\n",
    "        str +=f\"{row['link']} <{row['link']}>`| {row['storage_class']}|\\n\"\n",
    "    if row['storage_class'] == 'MetricMeasurementBundle':\n",
    "        str = f\"| [{task['pipelines_name']}]({task['pipelines_url']})|\"\n",
    "        str +=f\"[{row['original_name']}](#{row['link']})| {row['storage_class']}|\\n\"\n",
    "    return str\n",
    "\n",
    "\n",
    "def create_other_row(row):\n",
    "    str = \"||{ref}`\"\n",
    "    str +=f\"{row['link']} <{row['link']}>`| {row['storage_class']}|\\n\"\n",
    "    return str\n",
    "\n",
    "\n",
    "def create_task_tables_markdown(task_frame,task_table_dict):\n",
    "    header_row =  \"Task (link to pipeline yaml) | datasetType | StorageClass |\\n|---   |---   |---   |\\n\"\n",
    "    skip_row = \"|||\\n\"\n",
    "\n",
    "    visit_task_string = header_row\n",
    "    tract_task_string = header_row\n",
    "    for index, task in task_frame.iterrows():\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if task['type'] == 'visit':\n",
    "            rows_frame = task_table_dict[task['name']]\n",
    "            for i in range(rows_frame.shape[0]):\n",
    "                if i==0:\n",
    "                    visit_task_string += create_initial_row(task,rows_frame.iloc[i])\n",
    "                else:\n",
    "                    visit_task_string += create_other_row(rows_frame.iloc[i])\n",
    "            visit_task_string += skip_row   \n",
    "        elif task['type'] == 'tract':\n",
    "            rows_frame = task_table_dict[task['name']]\n",
    "            for i in range(rows_frame.shape[0]):\n",
    "                if i==0:\n",
    "                    tract_task_string += create_initial_row(task,rows_frame.iloc[i])\n",
    "                else:\n",
    "                    tract_task_string += create_other_row(rows_frame.iloc[i])\n",
    "            tract_task_string += skip_row \n",
    "        else:\n",
    "            raise Execption(\"bad task type\")\n",
    "            \n",
    "    # visit task table\n",
    "    print(\"visit task table\")\n",
    "    print(visit_task_string)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    print('tract task table')\n",
    "    print(tract_task_string)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "def create_plot_references(task_frame,task_table_dict):\n",
    "    for index, task in task_frame.iterrows():\n",
    "        print(task['name'])\n",
    "        print()\n",
    "        rows_frame = task_table_dict[task['name']]\n",
    "        for i in range(rows_frame.shape[0]):\n",
    "            row = rows_frame.iloc[i]\n",
    "            if row['storage_class'] == \"Plot\":\n",
    "                str =\"```{figure} \"\n",
    "                str +=f\"/_static/plots/{task['name']}/{row['link'] + '.png'}\\n\"\n",
    "                str +=f\":name: {row['link']}\\n\\n\"\n",
    "                str +=f\"**Plot Name**: {row['original_name']}\\n\\n\"\n",
    "                str +=f\"Associated metrics: [{task['name']}_metrics](#{task['name'].lower()}_metrics)\\n\"\n",
    "                str +=\"```\\n\\n\"\n",
    "                print(str)\n",
    "       # :name: row['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba910a46-7690-419e-9681-c3aa3e716f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plot_references(task_frame, task_table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8633227-afb7-4a38-83cf-b41afdd7ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_task_tables_markdown(task_frame, task_table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb4fe6-87ce-430d-a541-c7a82b56b711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cc253-1b79-4610-9c50-5470e7e60531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165349f-0f40-4181-a3ee-d1435f29a1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
